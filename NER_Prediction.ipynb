{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7714/3636913641.py:5: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data['Sentence #'] = data['Sentence #'].fillna(method='ffill')\n",
      "/tmp/ipykernel_7714/3636913641.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sentences = data.groupby('Sentence #').apply(lambda s: [(w, t) for w, t in zip(s['Word'].tolist(), s['Tag'].tolist())])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('ner_dataset.csv.gz', encoding='latin1')\n",
    "\n",
    "# preprocessing\n",
    "data['Sentence #'] = data['Sentence #'].fillna(method='ffill')\n",
    "\n",
    "\n",
    "sentences = data.groupby('Sentence #').apply(lambda s: [(w, t) for w, t in zip(s['Word'].tolist(), s['Tag'].tolist())])\n",
    "sentences = [s for s in sentences]\n",
    "\n",
    "# Train/Test and Validation set\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "train_sentences, val_sentences = train_test_split(train_sentences, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature creation\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = {\n",
    "        'word.lower()': str(word).lower(),\n",
    "        'word[-3:]': str(word)[-3:],\n",
    "        'word[-2:]': str(word)[-2:],\n",
    "        'word.isupper()': str(word).isupper(),\n",
    "        'word.istitle()': str(word).istitle(),\n",
    "        'word.isdigit()': str(word).isdigit(),\n",
    "        'BOS': i == 0,\n",
    "        'EOS': i == len(sent) - 1\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': str(word1).lower(),\n",
    "            '-1:word.istitle()': str(word1).istitle(),\n",
    "            '-1:word.isupper()': str(word1).isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': str(word1).lower(),\n",
    "            '+1:word.istitle()': str(word1).istitle(),\n",
    "            '+1:word.isupper()': str(word1).isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for word, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word for word, label in sent]\n",
    "\n",
    "X_train = [sent2features(s) for s in train_sentences]\n",
    "y_train = [sent2labels(s) for s in train_sentences]\n",
    "\n",
    "X_val = [sent2features(s) for s in val_sentences]\n",
    "y_val = [sent2labels(s) for s in val_sentences]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sentences]\n",
    "y_test = [sent2labels(s) for s in test_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer(sparse=True)\n",
    "\n",
    "X_train_flat = [item for sublist in X_train for item in sublist]\n",
    "y_train_flat = [item for sublist in y_train for item in sublist]\n",
    "\n",
    "X_val_flat = [item for sublist in X_val for item in sublist]\n",
    "y_val_flat = [item for sublist in y_val for item in sublist]\n",
    "\n",
    "X_test_flat = [item for sublist in X_test for item in sublist]\n",
    "y_test_flat = [item for sublist in y_test for item in sublist]\n",
    "\n",
    "X_train_vect = vec.fit_transform(X_train_flat)\n",
    "X_val_vect = vec.transform(X_val_flat)\n",
    "X_test_vect = vec.transform(X_test_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] END ..............................................C=0.1; total time=  48.8s\n",
      "[CV] END ..............................................C=0.1; total time=  51.1s\n",
      "[CV] END ..............................................C=0.1; total time=  51.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................................C=1; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................................C=1; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...............................................C=10; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................................C=1; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...............................................C=10; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...............................................C=10; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..............................................C=100; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..............................................C=100; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..............................................C=100; total time= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logreg_model.joblib']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression(max_iter=100)\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_vect, y_train_flat)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_logreg = grid_search.best_estimator_\n",
    "best_logreg.fit(X_train_vect, y_train_flat)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_logreg, 'logreg_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.30      0.03      0.06        94\n",
      "       B-eve       0.62      0.26      0.36        70\n",
      "       B-geo       0.85      0.91      0.88      7558\n",
      "       B-gpe       0.95      0.94      0.94      3142\n",
      "       B-nat       0.55      0.28      0.37        40\n",
      "       B-org       0.79      0.67      0.73      4151\n",
      "       B-per       0.82      0.81      0.81      3400\n",
      "       B-tim       0.92      0.86      0.89      4077\n",
      "       I-art       0.00      0.00      0.00        84\n",
      "       I-eve       0.64      0.22      0.32        65\n",
      "       I-geo       0.81      0.72      0.76      1462\n",
      "       I-gpe       0.94      0.52      0.67        33\n",
      "       I-nat       1.00      0.15      0.27        13\n",
      "       I-org       0.78      0.73      0.75      3394\n",
      "       I-per       0.83      0.87      0.85      3406\n",
      "       I-tim       0.85      0.69      0.76      1251\n",
      "           O       0.99      0.99      0.99    177590\n",
      "\n",
      "    accuracy                           0.97    209830\n",
      "   macro avg       0.74      0.57      0.61    209830\n",
      "weighted avg       0.97      0.97      0.97    209830\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_logreg.predict(X_test_vect)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test_flat, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass a sample sentence the get the predicted NER tag for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turkey: B-geo\n",
      "has: O\n",
      "sent: O\n",
      "more: O\n",
      "troops: O\n",
      "and: O\n",
      "tanks: O\n",
      "to: O\n",
      "the: O\n",
      "Iraqi: B-gpe\n",
      "border: O\n",
      ",: O\n",
      "as: O\n",
      "speculation: O\n",
      "grows: O\n",
      "about: O\n",
      "a: O\n",
      "possible: O\n",
      "Turkish: B-gpe\n",
      "incursion: O\n",
      "against: O\n",
      "Kurdish: O\n",
      "rebels: O\n",
      "in: O\n",
      "northern: O\n",
      "Iraq: B-geo\n",
      ".: O\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input_string(input_string, vec):\n",
    "    \"\"\"\n",
    "    Preprocess the input string and convert it to the format required by the model.\n",
    "    \"\"\"\n",
    "    # Tokenize the input string\n",
    "    tokens = input_string.split()\n",
    "    \n",
    "    # Extract features for each token\n",
    "    features = [word2features([(token, '')], 0) for token in tokens]\n",
    "    \n",
    "    # Convert features to vector\n",
    "    features_vector = vec.transform(features)\n",
    "    \n",
    "    return tokens, features_vector\n",
    "\n",
    "# Example input string\n",
    "input_string = 'Turkey has sent more troops and tanks to the Iraqi border , as speculation grows about a possible Turkish incursion against Kurdish rebels in northern Iraq .'\n",
    "\n",
    "# Preprocess the input string\n",
    "tokens, input_features = preprocess_input_string(input_string, vec)\n",
    "\n",
    "# Predict NER tags\n",
    "predicted_tags = best_logreg.predict(input_features)\n",
    "\n",
    "# Display the results\n",
    "for token, tag in zip(tokens, predicted_tags):\n",
    "    print(f\"{token}: {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Turkey has sent more troops and tanks to the Iraqi border , as speculation grows about a possible Turkish incursion against Kurdish rebels in northern Iraq .'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ' '.join([key for value in val_sentences[300:301]for key, val in value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Turkey', 'B-org'),\n",
       "  ('has', 'O'),\n",
       "  ('sent', 'O'),\n",
       "  ('more', 'O'),\n",
       "  ('troops', 'O'),\n",
       "  ('and', 'O'),\n",
       "  ('tanks', 'O'),\n",
       "  ('to', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('Iraqi', 'B-gpe'),\n",
       "  ('border', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('as', 'O'),\n",
       "  ('speculation', 'O'),\n",
       "  ('grows', 'O'),\n",
       "  ('about', 'O'),\n",
       "  ('a', 'O'),\n",
       "  ('possible', 'O'),\n",
       "  ('Turkish', 'B-gpe'),\n",
       "  ('incursion', 'O'),\n",
       "  ('against', 'O'),\n",
       "  ('Kurdish', 'O'),\n",
       "  ('rebels', 'O'),\n",
       "  ('in', 'O'),\n",
       "  ('northern', 'O'),\n",
       "  ('Iraq', 'B-geo'),\n",
       "  ('.', 'O')]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sentences[300:301]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: datasets in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (2.16.1)\n",
      "Requirement already satisfied: torch in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: sympy in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/umeshnagar/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install required Libraries \n",
    "! pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52781/1600698595.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = pd.read_csv('ner_dataset.csv.gz', encoding='latin1').fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('ner_dataset.csv.gz', encoding='latin1').fillna(method='ffill')\n",
    "\n",
    "# preprocessing\n",
    "tags_vals = list(set(data[\"Tag\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "# Call BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset\n",
    "def process_data(data):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    for index, row in data.iterrows():\n",
    "        if row['Word'] == '.':\n",
    "            sentences.append(words)\n",
    "            labels.append(tags)\n",
    "            words = []\n",
    "            tags = []\n",
    "        else:\n",
    "            words.append(row['Word'])\n",
    "            tags.append(row['Tag'])\n",
    "    return sentences, labels\n",
    "\n",
    "sentences, labels = process_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 12:43:32.795242: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 12:43:32.798286: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 12:43:32.834905: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 12:43:35.337822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "tags_encoded = [[tag2idx[tag] for tag in sent] for sent in labels]\n",
    "\n",
    "# Pad sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tags_padded = pad_sequences(tags_encoded, maxlen=MAX_LEN, dtype=\"long\", padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_inputs, test_inputs, train_tags, test_tags = train_test_split(sentences, tags_padded, random_state=42, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_inputs],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_inputs],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attention_masks = [[float(i != 0) for i in ii] for ii in train_input_ids]\n",
    "test_attention_masks = [[float(i != 0) for i in ii] for ii in test_input_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "train_inputs = torch.tensor(train_input_ids)\n",
    "train_tags = torch.tensor(train_tags)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_tags = torch.tensor(test_tags)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "# Create DataLoader for train set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(tag2idx))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped due to time limit.\n",
      "Epoch 1 completed. Total training time: 607.38s\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "\n",
    "# Maximum sequence length\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Define batch size\n",
    "bs = 16\n",
    "\n",
    "# Define epochs\n",
    "epochs = 1 # kept it lower so that it can run fast with min resource requirement (Caused Kernal to crash) \n",
    "import time\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model.zero_grad()\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"labels\": batch[2]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0 and step != 0:\n",
    "            print(f\"Epoch {epoch + 1}, Batch {step}, Loss: {total_loss / step:.4f}, Time: {time.time() - start_time:.2f}s\")\n",
    "        if time.time() - start_time > 600:  # Stop training if exceeding 10 minutes\n",
    "            print(\"Training stopped due to time limit.\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch + 1} completed. Total training time: {time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, true_labels = [], []\n",
    "for batch in test_dataloader:\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[0],\n",
    "        \"attention_mask\": batch[1]\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    logits = torch.argmax(logits, dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = batch[2].cpu().numpy()\n",
    "    predictions.extend([list(p) for p in logits])\n",
    "    true_labels.extend(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten predictions and true labels\n",
    "pred_tags = [tags_vals[p_i] for p, l in zip(predictions, true_labels)\n",
    "                             for p_i, l_i in zip(p, l) if tags_vals[l_i] != \"PAD\"]\n",
    "valid_tags = [tags_vals[l_i] for l in true_labels\n",
    "                              for l_i in l if tags_vals[l_i] != \"PAD\"]\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(valid_tags, pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample string\n",
    "sample_string = \"Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country.\"\n",
    "\n",
    "# Tokenize the sample string\n",
    "tokenized_input = tokenizer.encode(sample_string, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the tokenized input through the model\n",
    "with torch.no_grad():\n",
    "    output = model(tokenized_input)\n",
    "\n",
    "# Get the predicted tags\n",
    "predicted_tags = torch.argmax(output.logits, dim=2)[0].tolist()\n",
    "\n",
    "# Decode the predicted tags\n",
    "predicted_tags_decoded = [tags_vals[tag] for tag in predicted_tags]\n",
    "\n",
    "# Print the sample string and predicted tags\n",
    "print(\"Sample String:\", sample_string)\n",
    "print(\"Predicted Tags:\", predicted_tags_decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
